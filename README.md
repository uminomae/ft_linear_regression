# ft_linear_regression

## 微分の基本概念：

- 「傾き」を求めるため
- 微分は関数の「変化率」

線形関数 y = ax + b の場合、その導関数（微分した結果）は常に定数 a になります。
つまり、dy/dx = a です。

## 勾配降下法における傾きの計算：

- 実際のデータポイントと予測値の差（誤差）を最小化することが目標
- この誤差を表す関数を「損失関数」と呼ぶ
- 一般的に使われる損失関数の一つが「平均二乗誤差（MSE）」

### MSEの定義:

n個のデータポイント (xi, yi) に対して、  
MSE = (1/n) * Σ(yi - (axi + b))²  
ここで、Σは i=1 から n までの和を表します。 
この数式は、実際の値と予測値の差を二乗して平均を取っていることを表しています。  

- yi は実際のデータポイントのy値
- (axi + b) は線形関数による予測値
- (yi - (axi + b)) は実際の値と予測値の差（誤差）
- (yi - (axi + b))² は誤差を二乗した値
- (1/n) * をかけることで、合計した二乗誤差の平均を取ります

- 二乗を使う理由
  - 正の誤差と負の誤差を同等に扱えます
  - 大きな誤差により重みを置くことができます
  - 数学的に扱いやすくなります（微分が簡単になる）

## 偏微分の概念：

偏微分は、複数の変数を持つ関数において、一つの変数に注目してその変数に関する微分を行う

- 例えば、z = f(x, y) という2変数関数があるとき：  

xに関する偏微分：∂z/∂x は、yを定数として扱い、xだけを変数として微分します。  
yに関する偏微分：∂z/∂y は、xを定数として扱い、yだけを変数として微分します。  

- MSEの場合、aとbの2つのパラメータがあるので、それぞれに関して偏微分を行います：

∂MSE/∂a：bを固定して、aだけを変化させたときのMSEの変化率  
∂MSE/∂b：aを固定して、bだけを変化させたときのMSEの変化率  

## 傾きの計算：

- 勾配降下法では、この損失関数に対するパラメータ a と b の偏微分を計算  
  - 損失関数が a と b の変化に対してどのように変化するかを表す

∂MSE/∂a = (-2/n) * Σ(yi - (axi + b)) * xi  
∂MSE/∂b = (-2/n) * Σ(yi - (axi + b))  

- 式の導出
MSE = (1/n) * Σ(yi - (axi + b))²
これを a と b について偏微分

MSEを a で偏微分すると：
∂MSE/∂a = ∂/∂a [(1/n) * Σ(yi - (axi + b))²]
= (1/n) * Σ∂/∂a [(yi - (axi + b))²]
= (1/n) * Σ[2(yi - (axi + b)) * (-xi)]  （チェーンルールを適用）
= (-2/n) * Σ[(yi - (axi + b)) * xi]

b で偏微分すると：
∂MSE/∂b = ∂/∂b [(1/n) * Σ(yi - (axi + b))²]
= (1/n) * Σ∂/∂b [(yi - (axi + b))²]
= (1/n) * Σ[2(yi - (axi + b)) * (-1)]  （チェーンルールを適用）
= (-2/n) * Σ(yi - (axi + b))

## パラメータの更新：

- 計算された傾きを使って、a と b を以下のように更新  

a = a - η * (∂MSE/∂a)  
b = b - η * (∂MSE/∂b)  

ここで、η（イータ）は学習率と呼ばれる小さな正の数で、一度にどれだけパラメータを更新するかを制御します。  
このプロセスを繰り返すことで、徐々に最適な a と b の値に近づいていきます  

これらの偏微分を計算することで、aとbをそれぞれどう変更すればMSEが小さくなるかがわかります。
